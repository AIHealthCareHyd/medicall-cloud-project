<script type="text/babel">

        const { useState, useEffect, useRef } = React;

        function App() {
            const [conversation, setConversation] = useState([
                { role: 'model', text: "Welcome to Prudence Hospitals. I'm Sahay, your AI assistant. How can I help you today?" }
            ]);
            const [status, setStatus] = useState('idle'); // idle, listening, thinking, speaking
            const [isListeningActive, setIsListeningActive] = useState(false);
            
            const recognitionRef = useRef(null);
            const conversationEndRef = useRef(null);

            const scrollToBottom = () => {
                conversationEndRef.current?.scrollIntoView({ behavior: "smooth" });
            };

            useEffect(() => {
                scrollToBottom();
            }, [conversation]);

            const speak = (text) => {
                window.speechSynthesis.cancel();
                return new Promise((resolve, reject) => {
                    setStatus('speaking');
                    const utterance = new SpeechSynthesisUtterance(text);
                    const voices = window.speechSynthesis.getVoices();
                    if (voices.length > 0) {
                        const femaleVoice = voices.find(voice => voice.lang.startsWith('en') && (voice.gender === 'female' || voice.name.includes('Female')));
                        if (femaleVoice) utterance.voice = femaleVoice;
                    }
                    utterance.onend = () => {
                        setStatus('idle');
                        resolve();
                    };
                    utterance.onerror = (event) => {
                        console.error("SpeechSynthesis Error", event);
                        setStatus('idle');
                        reject(event.error);
                    };
                    window.speechSynthesis.speak(utterance);
                });
            };
            
            const getAiReply = async (currentConversation) => {
                setStatus('thinking');
                
                const historyForApi = currentConversation.map(turn => ({
                    role: turn.role,
                    parts: [{ text: turn.text }]
                }));

                const functionUrl = 'https://sahayhealth.netlify.app/.netlify/functions/getAiResponse';

                try {
                    const response = await fetch(functionUrl, {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({ history: historyForApi }),
                    });

                    if (!response.ok) {
                        const errorText = await response.text();
                        throw new Error(`Network error: ${response.status} - ${errorText}`);
                    }

                    const data = await response.json();
                    const aiReplyText = data.reply || "Sorry, I had trouble understanding that.";
                    
                    setConversation(prev => [...prev, { role: 'model', text: aiReplyText }]);
                    
                    await speak(aiReplyText);
                    
                    if (isListeningActive && recognitionRef.current) {
                        recognitionRef.current.start();
                    }

                } catch (error) {
                    console.error("Error fetching AI response:", error);
                    const errorText = `There was an error: ${error.message}`;
                    setConversation(prev => [...prev, { role: 'model', text: errorText }]);
                    await speak(errorText).catch(e => console.error("Error speaking the error message:", e));
                    if (isListeningActive && recognitionRef.current) {
                        recognitionRef.current.start();
                    }
                }
            };

            const toggleListening = () => {
                const currentlyActive = !isListeningActive;
                setIsListeningActive(currentlyActive);

                if (currentlyActive) {
                    if (recognitionRef.current) {
                        recognitionRef.current.start();
                    }
                } else {
                    if (recognitionRef.current) {
                        recognitionRef.current.stop();
                    }
                }
            };

            // --- MAIN FIX IS HERE ---
            useEffect(() => {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (!SpeechRecognition) {
                    alert("Sorry, your browser doesn't support speech recognition.");
                    return;
                }

                if (!recognitionRef.current) {
                    recognitionRef.current = new SpeechRecognition();
                    recognitionRef.current.continuous = true;
                    recognitionRef.current.interimResults = false;
                }
                
                const recognition = recognitionRef.current;

                recognition.onstart = () => {
                    setStatus('listening');
                };

                // UPDATED LOGIC: This handler is now the key to making the session robust.
                recognition.onend = () => {
                    // Check if we should be listening. If the master toggle is on, but we are not currently
                    // processing a request (thinking/speaking), it means recognition stopped on its own
                    // due to a browser timeout. We should restart it to keep the session alive.
                    if (isListeningActive && status !== 'thinking' && status !== 'speaking') {
                        console.log("Recognition timed out, restarting...");
                        try {
                            recognition.start();
                        } catch (e) {
                            console.error("Error restarting recognition onend:", e);
                            setStatus('idle');
                        }
                    } else if (!isListeningActive) {
                        // This handles the case where the user manually clicked "Stop Listening".
                        setStatus('idle');
                    }
                    // If isListeningActive is true BUT status is 'thinking' or 'speaking', we do nothing here.
                    // The restart is correctly handled by the getAiReply function after the AI finishes speaking.
                };

                recognition.onerror = (event) => {
                    console.error("Speech recognition error:", event.error);
                    // The 'no-speech' error is common if the user is silent. The onend event will
                    // handle the restart, so we don't need to do much here.
                    if (event.error !== 'no-speech' && event.error !== 'aborted') {
                        setStatus('idle');
                    }
                };
                
                recognition.onresult = (event) => {
                    const transcript = event.results[event.results.length - 1][0].transcript.trim();

                    if(transcript) {
                        // Stop recognition temporarily while we process and get a reply.
                        // This prevents it from listening to the AI's response.
                        recognition.stop();
                        
                        setConversation(prevConversation => {
                            const updatedConversation = [...prevConversation, { role: 'user', text: transcript }];
                            getAiReply(updatedConversation); // This function will restart recognition when it's done.
                            return updatedConversation;
                        });
                    }
                };

                return () => {
                    recognition.onstart = null;
                    recognition.onend = null;
                    recognition.onerror = null;
                    recognition.onresult = null;
                };
            // This useEffect now depends on 'status' to ensure the 'onend' handler always has the latest state.
            }, [isListeningActive, status]);


            useEffect(() => {
                const warmUp = () => window.speechSynthesis.getVoices();
                if (window.speechSynthesis.onvoiceschanged !== undefined) {
                    window.speechSynthesis.onvoiceschanged = warmUp;
                }
                warmUp();
                return () => { window.speechSynthesis.onvoiceschanged = null; };
            }, []);

            const getButtonTextAndStyle = () => {
                if (isListeningActive) {
                    if (status === 'thinking' || status === 'speaking') {
                        return { text: status.charAt(0).toUpperCase() + status.slice(1) + '...', style: "bg-gray-400 cursor-not-allowed"};
                    }
                    return { text: 'Stop Listening', style: "bg-red-500 hover:bg-red-600" };
                }
                return { text: 'Talk to Sahay', style: "bg-orange-500 hover:bg-orange-600" };
            };

            const { text: buttonText, style: buttonStyle } = getButtonTextAndStyle();

            return (
                <div className="flex flex-col items-center justify-center min-h-screen text-gray-800 font-sans p-4 bg-gray-50">
                    <div className="w-full max-w-md p-6 bg-white rounded-lg shadow-xl">
                        <div className="text-center mb-4">
                            <h1 className="text-3xl font-bold text-blue-900">PRUDENCE</h1>
                            <p className="text-sm tracking-widest text-blue-800">HOSPITALS</p>
                        </div>
                        
                        <h2 className="text-xl font-semibold text-center text-gray-700 mb-2">Health Assistant</h2>
                        <p className="text-center text-gray-500 mb-6">Your AI Voice Assistant</p>
                        
                        <div className="p-4 bg-gray-100 rounded-md h-[300px] mb-4 overflow-y-auto flex flex-col gap-2">
                            {conversation.map((turn, index) => (
                                <div key={index} className={`p-3 rounded-lg max-w-[85%] ${turn.role === 'user' ? 'bg-gray-200 self-end' : 'bg-blue-100 self-start'}`}>
                                    <p>{turn.text}</p>
                                </div>
                            ))}
                             <div ref={conversationEndRef} />
                        </div>
                        
                        <div className="flex justify-center mt-4">
                            <button
                                onClick={toggleListening}
                                disabled={isListeningActive && (status === 'thinking' || status === 'speaking')}
                                className={`px-8 py-4 text-white font-bold rounded-full shadow-lg transition-all duration-300 disabled:cursor-not-allowed ${buttonStyle}`}
                            >
                                {buttonText}
                            </button>
                        </div>
                    </div>
                </div>
            );
        }

        const container = document.getElementById('root');
        const root = ReactDOM.createRoot(container);
        root.render(<App />);

    </script>